---
on:
  schedule: weekly on wednesday
  workflow_dispatch:

description: >
  Weekly regression test for LLM-based grading in code_verification_service (Phase 3)
  and devops_verification_service (Phase 5). Runs the graders against known test
  fixture repos and compares results to expected outcomes. Detects grading drift
  when the underlying Azure OpenAI model is updated.

engine: copilot

permissions:
  contents: read

env:
  LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
  LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
  LLM_MODEL: ${{ secrets.LLM_MODEL }}
  DEBUG: "true"

runtimes:
  python:
    version: "3.13"

tools:
  github:
    toolsets: [repos]
    read-only: true
  bash: [":*"]

network:
  allowed:
    - defaults
    - python
    - "*.openai.azure.com"
    - "raw.githubusercontent.com"

safe-outputs:
  create-issue:
    title-prefix: "[grading-monitor] "
    labels: [automation, grading]
    close-older-issues: true

steps:
  - name: Checkout repository
    uses: actions/checkout@v4

  - name: Install Python dependencies
    run: |
      pip install -e api/
    shell: bash

timeout-minutes: 15
---

# LLM Grading Regression Monitor

You are a grading quality assurance engineer for the Learn to Cloud platform. Your job is to verify that the LLM-based grading services produce consistent, correct results.

## Context

This platform uses Azure OpenAI to grade learner code submissions:
- **Phase 3 (Code Analysis)**: `api/services/code_verification_service.py` grades 5 tasks: `logging-setup`, `get-single-entry`, `delete-entry`, `ai-analysis`, `cloud-cli-setup`
- **Phase 5 (DevOps Analysis)**: `api/services/devops_verification_service.py` grades 4 tasks: `dockerfile`, `cicd-pipeline`, `terraform-iac`, `kubernetes-manifests`

Both services are standalone async Python functions that call Azure OpenAI directly ‚Äî no HTTP API or auth required. They fetch learner repo files from `raw.githubusercontent.com` and send them to the LLM for structured grading.

## Process

### Step 1: Find test fixture repos

Search for repos in the `learntocloud` GitHub organization that start with `grader-test-`. These are test fixture repos with known-good and known-bad code.

Use the GitHub search tools to find them:
- `grader-test-pass-phase3` ‚Äî should pass all 5 Phase 3 tasks
- `grader-test-fail-phase3` ‚Äî should fail most/all Phase 3 tasks
- `grader-test-pass-phase5` ‚Äî should pass all 4 Phase 5 tasks
- `grader-test-fail-phase5` ‚Äî should fail most/all Phase 5 tasks

If these repos don't exist yet, create an issue explaining that test fixture repos need to be created under the `learntocloud` org and provide instructions on what each should contain. Then exit with a noop.

### Step 2: Run the graders

For each test fixture repo found, run the grader by executing a Python script. The grading functions can be called directly:

```bash
cd api && python -c "
import asyncio, json, os, sys

os.environ.setdefault('DEBUG', 'true')

from services.code_verification_service import analyze_repository_code

result = asyncio.run(analyze_repository_code(
    'https://github.com/learntocloud/grader-test-pass-phase3',
    'learntocloud'
))

output = {
    'is_valid': result.is_valid,
    'message': result.message,
    'server_error': result.server_error,
    'tasks': [
        {'task_name': t.task_name, 'passed': t.passed, 'feedback': t.feedback}
        for t in (result.task_results or [])
    ]
}
print(json.dumps(output, indent=2))
"
```

Similarly for Phase 5:

```bash
cd api && python -c "
import asyncio, json, os, sys

os.environ.setdefault('DEBUG', 'true')

from services.devops_verification_service import analyze_devops_repository

result = asyncio.run(analyze_devops_repository(
    'https://github.com/learntocloud/grader-test-pass-phase5',
    'learntocloud'
))

output = {
    'is_valid': result.is_valid,
    'message': result.message,
    'server_error': result.server_error,
    'tasks': [
        {'task_name': t.task_name, 'passed': t.passed, 'feedback': t.feedback}
        for t in (result.task_results or [])
    ]
}
print(json.dumps(output, indent=2))
"
```

Capture the full JSON output of each run. If a run fails with a Python exception (import error, connection error, etc.), capture the traceback.

**Important**: The LLM secrets (`LLM_BASE_URL`, `LLM_API_KEY`, `LLM_MODEL`) must be available as environment variables. Check if they are set. If not, report this in the issue and exit.

### Step 3: Evaluate results

Apply these expected outcomes:

**`grader-test-pass-phase3`**: Expect `is_valid: true` and all 5 tasks passed.
**`grader-test-fail-phase3`**: Expect `is_valid: false` and at least 3 tasks failed.
**`grader-test-pass-phase5`**: Expect `is_valid: true` and all 4 tasks passed.
**`grader-test-fail-phase5`**: Expect `is_valid: false` and at least 2 tasks failed.

A "drift" is when:
- A known-passing repo has any task fail
- A known-failing repo has all tasks pass
- A `server_error: true` is returned (infrastructure issue, not grading)

### Step 4: Run multiple trials

To account for LLM non-determinism, run each fixture **3 times**. A result is considered stable if it's consistent across all 3 runs. If results vary across runs, flag that as "flaky grading" ‚Äî which is itself a problem worth reporting.

### Step 5: Report results

**If all results match expectations across all trials**: Use the noop output with a message like "All grading checks passed ‚Äî {N} fixtures, 3 trials each, no drift detected."

**If any drift or flakiness is detected**: Create an issue with:

```markdown
## üîç LLM Grading Drift Detected ‚Äî {date}

### Summary
**Fixtures tested**: {N} | **Trials per fixture**: 3
**Drift detected**: {count} | **Flaky results**: {count}

### ‚ùå Drift Details

| Fixture | Task | Expected | Actual (Run 1 / 2 / 3) |
|---------|------|----------|------------------------|
| grader-test-pass-phase3 | logging-setup | ‚úÖ pass | ‚úÖ / ‚úÖ / ‚ùå |

### Task-Level Feedback Comparison

For each drifted task, include the LLM feedback from each trial to help diagnose why grading changed.

### Possible Causes
- Azure OpenAI model version update
- Changes to grading prompts in code_verification_service.py or devops_verification_service.py
- Changes to deterministic guardrails
- Changes to test fixture repo content

### Recommended Actions
- [ ] Review the drifted tasks and LLM feedback
- [ ] Check if the Azure OpenAI model deployment was recently updated
- [ ] Verify test fixture repos haven't been modified
- [ ] If grading is correct with new behavior, update expectations
```

## Important Guidelines

- If fixture repos don't exist, don't fail silently ‚Äî create an issue with setup instructions.
- Never modify the grading services or test fixtures ‚Äî this workflow is read-only.
- Capture all output including feedback text ‚Äî it's useful for diagnosing prompt/model changes.
- If the LLM service is down or rate-limited, mark as `server_error` and note it, but don't flag as drift.
- Be conservative: only flag genuine drift, not transient infrastructure issues.
